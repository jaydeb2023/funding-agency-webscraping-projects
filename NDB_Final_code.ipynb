{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54671b0-3524-49e5-aa43-e216cb0b1abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://www.ndb.int/projects/all-projects/page/1/\n",
      "Scraping page 2: https://www.ndb.int/projects/all-projects/page/2/\n",
      "Scraping page 3: https://www.ndb.int/projects/all-projects/page/3/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "BASE_URL = \"https://www.ndb.int/projects/all-projects/page/{}/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                   \"(KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Referer\": \"https://www.ndb.int/\"\n",
    "}\n",
    "\n",
    "def scrape_projects_from_page(page_num):\n",
    "    url = BASE_URL.format(page_num)\n",
    "    print(f\"Scraping page {page_num}: {url}\")\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    project_cards = soup.find_all(\"div\", class_=\"project-card card card-transition\")\n",
    "    if not project_cards:\n",
    "        print(\"No projects found on this page.\")\n",
    "        return []\n",
    "\n",
    "    projects = []\n",
    "    for card in project_cards:\n",
    "        sector_div = card.find(\"div\", class_=\"project-card-cat\")\n",
    "        sector = sector_div.text.strip() if sector_div else \"\"\n",
    "\n",
    "        country_div = card.find(\"div\", class_=\"project-card-country\")\n",
    "        country = country_div.text.strip() if country_div else \"\"\n",
    "\n",
    "        title_div = card.find(\"div\", class_=\"project-card-title\")\n",
    "        a_tag = title_div.find(\"a\") if title_div else None\n",
    "        project_name = a_tag.text.strip() if a_tag else \"\"\n",
    "        project_url = a_tag['href'] if a_tag else \"\"\n",
    "\n",
    "        type_div = card.find(\"div\", class_=\"project-card-type\")\n",
    "        project_type = type_div.text.strip() if type_div else \"\"\n",
    "\n",
    "        date_div = card.find(\"div\", class_=\"project-card-date\")\n",
    "        date = date_div.text.strip() if date_div else \"\"\n",
    "\n",
    "        projects.append({\n",
    "            \"Project Name\": project_name,\n",
    "            \"Project URL\": project_url,\n",
    "            \"Country\": country,\n",
    "            \"Sector\": sector,\n",
    "            \"Project Type\": project_type,\n",
    "            \"Date\": date\n",
    "        })\n",
    "    return projects\n",
    "\n",
    "def scrape_all_projects(start_page=1, end_page=12, delay=1):\n",
    "    all_projects = []\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        projects = scrape_projects_from_page(page_num)\n",
    "        if not projects:\n",
    "            print(f\"No projects on page {page_num}, stopping early.\")\n",
    "            break\n",
    "        all_projects.extend(projects)\n",
    "        time.sleep(delay)\n",
    "    return all_projects\n",
    "\n",
    "def get_pdf_link(project_url):\n",
    "    try:\n",
    "        response = requests.get(project_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        link = soup.find(\"a\", class_=\"btn primary card-link download-icon external\")\n",
    "        if link and 'href' in link.attrs:\n",
    "            return link['href']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching PDF link from {project_url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_pdf_text(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, headers=HEADERS, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        pdf_data = response.content\n",
    "        doc = fitz.open(stream=pdf_data, filetype=\"pdf\")\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF {pdf_url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_pdf_text_flexible(text):\n",
    "    fields = [\n",
    "        \"Project Name\",\n",
    "        \"Country\",\n",
    "        \"Type\",\n",
    "        \"Area of Operation\",\n",
    "        \"Concept Approval Date\",\n",
    "        \"Total Project Cost\",\n",
    "        \"Proposed Limit of NDB Financing\",\n",
    "        \"Borrower\",\n",
    "        \"Project Entity\",\n",
    "        \"Project Context\",\n",
    "        \"Project Objective\",\n",
    "        \"Project Description\"\n",
    "    ]\n",
    "    \n",
    "    parsed = {}\n",
    "    for i, field in enumerate(fields):\n",
    "        next_fields = fields[i+1:] if i+1 < len(fields) else []\n",
    "        pattern = rf\"{re.escape(field)}\\s*(.*?)(?=\" + \"|\".join([re.escape(f) for f in next_fields]) + \"|$)\"\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        parsed[field] = match.group(1).strip().replace('\\n', ' ') if match else None\n",
    "    return parsed\n",
    "\n",
    "def step1_scrape_projects_to_excel(start_page=1, end_page=12, output_excel=\"ndb_all_projects.xlsx\"):\n",
    "    projects = scrape_all_projects(start_page, end_page)\n",
    "    df = pd.DataFrame(projects)\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Step 1 complete: Scraped {len(projects)} projects and saved to '{output_excel}'\")\n",
    "    return output_excel\n",
    "\n",
    "def step2_add_pdf_links_and_text(input_excel, output_csv=\"ndb_projects_with_pdf_text.csv\", delay=1):\n",
    "    df = pd.read_excel(input_excel)\n",
    "    if 'Project URL' not in df.columns:\n",
    "        print(\"Error: 'Project URL' column not found in Excel.\")\n",
    "        return None\n",
    "    \n",
    "    pdf_urls = []\n",
    "    pdf_text_snippets = []\n",
    "    \n",
    "    for i, project_url in enumerate(df['Project URL'], 1):\n",
    "        print(f\"[{i}/{len(df)}] Processing project URL: {project_url}\")\n",
    "        pdf_link = get_pdf_link(project_url)\n",
    "        pdf_urls.append(pdf_link if pdf_link else \"\")\n",
    "        \n",
    "        pdf_text = \"\"\n",
    "        if pdf_link:\n",
    "            pdf_text = extract_pdf_text(pdf_link)\n",
    "        pdf_text_snippets.append(pdf_text[:1000] if pdf_text else \"\")\n",
    "        \n",
    "        time.sleep(delay)\n",
    "    \n",
    "    df['PDF URL'] = pdf_urls\n",
    "    df['PDF Text Snippet'] = pdf_text_snippets\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Step 2 complete: Added PDF URLs and text snippets saved to '{output_csv}'\")\n",
    "    return output_csv\n",
    "\n",
    "def step3_parse_pdf_text_to_excel(input_csv, output_excel=\"ndb_projects_parsed.xlsx\", delay=1):\n",
    "    if not os.path.isfile(input_csv):\n",
    "        print(f\"Input file not found: {input_csv}\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    if 'PDF URL' not in df.columns:\n",
    "        print(\"Error: 'PDF URL' column missing in input CSV.\")\n",
    "        return\n",
    "    \n",
    "    parsed_rows = []\n",
    "    total = len(df)\n",
    "    \n",
    "    for i, pdf_url in enumerate(df['PDF URL'], 1):\n",
    "        if pd.isna(pdf_url) or not pdf_url.strip():\n",
    "            print(f\"[{i}/{total}] Skipping empty PDF URL\")\n",
    "            parsed_rows.append({field: None for field in [\n",
    "                \"Project Name\",\"Country\",\"Type\",\"Area of Operation\",\"Concept Approval Date\",\n",
    "                \"Total Project Cost\",\"Proposed Limit of NDB Financing\",\"Borrower\",\n",
    "                \"Project Entity\",\"Project Context\",\"Project Objective\",\"Project Description\"\n",
    "            ]})\n",
    "            continue\n",
    "        \n",
    "        print(f\"[{i}/{total}] Parsing PDF: {pdf_url}\")\n",
    "        text = extract_pdf_text(pdf_url)\n",
    "        if text:\n",
    "            parsed = parse_pdf_text_flexible(text)\n",
    "        else:\n",
    "            parsed = {field: None for field in [\n",
    "                \"Project Name\",\"Country\",\"Type\",\"Area of Operation\",\"Concept Approval Date\",\n",
    "                \"Total Project Cost\",\"Proposed Limit of NDB Financing\",\"Borrower\",\n",
    "                \"Project Entity\",\"Project Context\",\"Project Objective\",\"Project Description\"\n",
    "            ]}\n",
    "        parsed_rows.append(parsed)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    parsed_df = pd.DataFrame(parsed_rows)\n",
    "    final_df = pd.concat([df.reset_index(drop=True), parsed_df.reset_index(drop=True)], axis=1)\n",
    "    final_df.to_excel(output_excel, index=False)\n",
    "    print(f\"Step 3 complete: Parsed PDF text saved to '{output_excel}'\")\n",
    "\n",
    "def main():\n",
    "    # Adjust page ranges or file paths if needed\n",
    "    excel_path = step1_scrape_projects_to_excel(start_page=1, end_page=12, output_excel=\"ndb_all_projects.xlsx\")\n",
    "    csv_path = step2_add_pdf_links_and_text(input_excel=excel_path, output_csv=\"ndb_projects_with_pdf_text.csv\")\n",
    "    if csv_path:\n",
    "        step3_parse_pdf_text_to_excel(input_csv=csv_path, output_excel=\"ndb_projects_parsed.xlsx\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2b967-d645-4e5e-9993-b1375e81c8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
